{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import librosa\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import os\n",
    "#from models.model_architecture import DemucsModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset directory\n",
    "dataset_dir = \"/path/to/dataset/\"\n",
    "batch_size = 3  # You can change this value as needed\n",
    "segment_length =10*44100 # Fixed length for all audio samples (~1.5 sec at 44.1kHz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get file paths for each category\n",
    "def get_file_paths(folder_name):\n",
    "    path = os.path.join(dataset_dir, folder_name)\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Warning: {folder_name} folder not found!\")\n",
    "        return []\n",
    "    return sorted([os.path.join(path, f) for f in os.listdir(path)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_paths = get_file_paths(\"song\")\n",
    "bass_paths = get_file_paths(\"bass\")\n",
    "vocal_paths = get_file_paths(\"vocal\")\n",
    "drum_paths = get_file_paths(\"drum\")\n",
    "music_paths = get_file_paths(\"music\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "def load_audio(file_path, target_sr=44100, segment_length=441000):  \n",
    "    audio, sr = librosa.load(file_path, sr=target_sr, mono=True)\n",
    "    \n",
    "    # Pad if audio is shorter than segment length\n",
    "    if len(audio) < segment_length:\n",
    "        audio = np.pad(audio, (0, segment_length - len(audio)), mode='constant')\n",
    "\n",
    "    # Number of segments (each 10 seconds long)\n",
    "    num_segments = len(audio) // segment_length  \n",
    "    segments = []\n",
    "    max_amplitudes = []\n",
    "\n",
    "    for i in range(num_segments):\n",
    "        start = i * segment_length\n",
    "        end = start + segment_length\n",
    "        segment = audio[start:end]\n",
    "\n",
    "        # Get max amplitude of the segment\n",
    "        original_max = np.max(np.abs(segment)) if np.max(np.abs(segment)) > 0 else 1.0  \n",
    "        max_amplitudes.append(original_max)\n",
    "\n",
    "        # Normalize the segment\n",
    "        segment = segment / original_max if original_max > 0 else segment\n",
    "        \n",
    "        segments.append(segment)\n",
    "\n",
    "    return np.array(segments), np.array(max_amplitudes)  # Return normalized segments + original max amplitudes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All segments have matching shapes\n"
     ]
    }
   ],
   "source": [
    "# Ensure all segments have the same shape\n",
    "for i in range(5):  # Checking first 5 samples\n",
    "    assert train_data[i][0].shape == train_data[i][1].shape == train_data[i][2].shape == train_data[i][3].shape == train_data[i][4].shape, f\"Mismatch at index {i}\"\n",
    "print(\"All segments have matching shapes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure dataset length consistency\n",
    "min_len = min(len(song_paths), len(bass_paths), len(vocal_paths), len(drum_paths), len(music_paths))\n",
    "print(f\"Using {min_len} samples for training.\")\n",
    "song_paths, bass_paths, vocal_paths, drum_paths, music_paths = (\n",
    "    song_paths[:min_len], bass_paths[:min_len], vocal_paths[:min_len], drum_paths[:min_len], music_paths[:min_len]\n",
    ")\n",
    "\n",
    "# Prepare dataset\n",
    "train_data = []  # Stores only the audio segments for training\n",
    "max_amplitudes = []  # Stores original max amplitudes for reconstruction\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "for i in range(min_len):\n",
    "    song_segments, song_max = load_audio(song_paths[i])\n",
    "    bass_segments, bass_max = load_audio(bass_paths[i])\n",
    "    vocal_segments, vocal_max = load_audio(vocal_paths[i])\n",
    "    drum_segments, drum_max = load_audio(drum_paths[i])\n",
    "    music_segments, music_max = load_audio(music_paths[i])\n",
    "\n",
    "    for j in range(len(song_segments)):  # Loop over segments\n",
    "        train_data.append((\n",
    "            song_segments[j], \n",
    "            bass_segments[j], \n",
    "            vocal_segments[j], \n",
    "            drum_segments[j], \n",
    "            music_segments[j]\n",
    "        ))  \n",
    "\n",
    "        # Store max amplitudes separately\n",
    "        max_amplitudes.append((\n",
    "            song_max[j],  \n",
    "            bass_max[j],  \n",
    "            vocal_max[j],  \n",
    "            drum_max[j],  \n",
    "            music_max[j]\n",
    "        ))\n",
    "\n",
    "print(\"Dataset loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training and testing\n",
    "train_set, test_set = train_test_split(train_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert lists to PyTorch tensors\n",
    "train_dataset = TensorDataset(*[torch.tensor(np.array(d), dtype=torch.float32) for d in zip(*train_set)])\n",
    "test_dataset = TensorDataset(*[torch.tensor(np.array(d), dtype=torch.float32) for d in zip(*test_set)])\n",
    "\n",
    "# Prepare DataLoaders\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape: [torch.Size([3, 441000]), torch.Size([3, 441000]), torch.Size([3, 441000]), torch.Size([3, 441000]), torch.Size([3, 441000])]\n"
     ]
    }
   ],
   "source": [
    "# Fetch one batch from DataLoader and check its shape\n",
    "for batch in train_data_loader:\n",
    "    print(f\"Batch shape: {[b.shape for b in batch]}\")\n",
    "    break  # Print only first batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total samples in dataset: {len(train_data)}\")  \n",
    "print(f\"Each sample should have 5 elements (song, bass, vocal, drum, music): {len(train_data[0])}\")  \n",
    "\n",
    "# Print the shape of a few segments\n",
    "print(\"Example shapes:\")\n",
    "print(f\"Song segment shape: {train_data[0][0].shape}\")\n",
    "print(f\"Bass segment shape: {train_data[0][1].shape}\")\n",
    "print(f\"Vocal segment shape: {train_data[0][2].shape}\")\n",
    "print(f\"Drum segment shape: {train_data[0][3].shape}\")\n",
    "print(f\"Music segment shape: {train_data[0][4].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Constants\n",
    "num_sources = 4  # bass, vocal, drums, music\n",
    "\n",
    "class DemucsModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DemucsModel, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(1, 32, kernel_size=16, stride=4, padding=8),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, 64, kernel_size=16, stride=4, padding=8),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(64, 128, kernel_size=16, stride=4, padding=8),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(128, 256, kernel_size=16, stride=8, padding=8),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Bidirectional LSTM for temporal modeling\n",
    "        self.rnn = nn.LSTM(256, 256, batch_first=True, bidirectional=True)\n",
    "        self.lstm_fc = nn.Linear(512, 256)  # Merge bidirectional outputs\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose1d(256, 128, kernel_size=16, stride=8, padding=8, output_padding=2),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(128, 64, kernel_size=16, stride=4, padding=8, output_padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(64, 32, kernel_size=16, stride=4, padding=8, output_padding=1),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(32, num_sources, kernel_size=16, stride=4, padding=8, output_padding=1),\n",
    "            nn.Tanh(),  # Keeps output in range (-1, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)  \n",
    "        x = x.permute(0, 2, 1)  # Change to (batch, time, channels) for LSTM\n",
    "        x, _ = self.rnn(x)  \n",
    "        x = self.lstm_fc(x)  \n",
    "        x = self.decoder(x.permute(0, 2, 1))  # Change back to (batch, channels, time)\n",
    "        return x\n",
    "\n",
    "# Initialize Model\n",
    "model = DemucsModel()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def si_snr_loss(pred, target, eps=1e-8):\n",
    "    target_energy = torch.sum(target**2, dim=-1, keepdim=True) + eps\n",
    "    scale = torch.sum(target * pred, dim=-1, keepdim=True) / target_energy\n",
    "    target_proj = scale * target\n",
    "    noise = pred - target_proj\n",
    "\n",
    "    si_snr = torch.sum(target_proj**2, dim=-1) / (torch.sum(noise**2, dim=-1) + eps)\n",
    "    si_snr = 10 * torch.log10(si_snr + eps)\n",
    "    \n",
    "    return -si_snr.mean()\n",
    "\n",
    "criterion = si_snr_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 1/50, Loss: 41.4152\n",
      "Epoch 2/50, Loss: 22.2007\n",
      "Epoch 3/50, Loss: 17.3469\n",
      "Epoch 4/50, Loss: 14.9297\n",
      "Epoch 5/50, Loss: 15.1850\n",
      "Epoch 6/50, Loss: 14.1816\n",
      "Epoch 7/50, Loss: 12.8669\n",
      "Epoch 8/50, Loss: 11.8511\n",
      "Epoch 9/50, Loss: 10.8245\n",
      "Epoch 10/50, Loss: 10.4349\n",
      "Epoch 11/50, Loss: 9.6924\n",
      "Epoch 12/50, Loss: 10.0983\n",
      "Epoch 13/50, Loss: 9.5579\n",
      "Epoch 14/50, Loss: 8.9316\n",
      "Epoch 15/50, Loss: 8.6701\n",
      "Epoch 16/50, Loss: 8.1620\n",
      "Epoch 17/50, Loss: 8.2290\n",
      "Epoch 18/50, Loss: 8.0776\n",
      "Epoch 19/50, Loss: 7.5438\n",
      "Epoch 20/50, Loss: 7.3371\n",
      "Epoch 21/50, Loss: 7.0777\n",
      "Epoch 22/50, Loss: 7.0217\n",
      "Epoch 23/50, Loss: 6.6650\n",
      "Epoch 24/50, Loss: 6.6382\n",
      "Epoch 25/50, Loss: 6.3798\n",
      "Epoch 26/50, Loss: 6.0281\n",
      "Epoch 27/50, Loss: 6.2811\n",
      "Epoch 28/50, Loss: 6.0460\n",
      "Epoch 29/50, Loss: 5.7686\n",
      "Epoch 30/50, Loss: 5.6906\n",
      "Epoch 31/50, Loss: 5.8909\n",
      "Epoch 32/50, Loss: 5.8145\n",
      "Epoch 33/50, Loss: 5.4752\n",
      "Epoch 34/50, Loss: 5.4045\n",
      "Epoch 35/50, Loss: 4.9797\n",
      "Epoch 36/50, Loss: 4.7261\n",
      "Epoch 37/50, Loss: 4.4821\n",
      "Epoch 38/50, Loss: 4.5373\n",
      "Epoch 39/50, Loss: 4.4469\n",
      "Epoch 40/50, Loss: 3.9839\n",
      "Epoch 41/50, Loss: 3.9756\n",
      "Epoch 42/50, Loss: 3.7086\n",
      "Epoch 43/50, Loss: 3.6296\n",
      "Epoch 44/50, Loss: 3.5454\n",
      "Epoch 45/50, Loss: 3.4562\n",
      "Epoch 46/50, Loss: 3.3326\n",
      "Epoch 47/50, Loss: 2.9741\n",
      "Epoch 48/50, Loss: 3.0594\n",
      "Epoch 49/50, Loss: 2.9747\n",
      "Epoch 50/50, Loss: 3.0975\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "target_length = 441000  # Ensure all outputs are 441000 samples\n",
    "\n",
    "print(\"Starting training...\")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    for batch_idx, batch in enumerate(train_data_loader):\n",
    "        inputs, target_bass, target_vocal, target_drum, target_music = batch\n",
    "        inputs = inputs.unsqueeze(1)  # Add channel dimension for Conv1d\n",
    "\n",
    "        # Pad inputs to match target length\n",
    "        inputs = F.pad(inputs, (0, target_length - inputs.shape[-1]))  \n",
    "        target_bass = F.pad(target_bass, (0, target_length - target_bass.shape[-1]))\n",
    "        target_vocal = F.pad(target_vocal, (0, target_length - target_vocal.shape[-1]))\n",
    "        target_drum = F.pad(target_drum, (0, target_length - target_drum.shape[-1]))\n",
    "        target_music = F.pad(target_music, (0, target_length - target_music.shape[-1]))\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)  # Model output: [batch_size, 4, ?] (unknown length)\n",
    "\n",
    "        # If model output is smaller, pad it to match target length\n",
    "        if outputs.shape[-1] < target_length:\n",
    "            pad_size = target_length - outputs.shape[-1]\n",
    "            outputs = F.pad(outputs, (0, pad_size))\n",
    "\n",
    "        # Stack targets for simpler loss computation\n",
    "        targets = torch.stack([target_bass, target_vocal, target_drum, target_music], dim=1)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Print batch info\n",
    "        #print(f\"Batch {batch_idx} - Predicted Shape: {outputs.shape}, Target Shape: {targets.shape}\")\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(train_data_loader):.4f}\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted shape: torch.Size([2, 4, 441000])\n",
      "Target shape: torch.Size([2, 441000])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Predicted shape: {outputs.shape}\")\n",
    "print(f\"Target shape: {target_bass.shape}\")  # Target shape should be same as predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"model_save.pth\")\n",
    "print(\"Model saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def remove_low_amplitude_noise(audio, threshold_ratio=0.07):\n",
    "    max_amplitude = np.max(np.abs(audio))\n",
    "    threshold = max_amplitude * threshold_ratio\n",
    "    audio_denoised = np.where(np.abs(audio) > threshold, audio, 0)\n",
    "    return audio_denoised\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting validation...\n",
      "Validation Loss: 36.3526\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import soundfile as sf\n",
    "\n",
    "print(\"Starting validation...\")\n",
    "model.eval()  # Set model to evaluation mode\n",
    "val_loss = 0\n",
    "output_dir = \"C:/Users/Mora siri/Desktop/output_audio_files_seconds/\"  # Output directory\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "with torch.no_grad():  # No need to compute gradients during validation\n",
    "    for batch_idx, batch in enumerate(test_data_loader):\n",
    "        inputs, target_bass, target_vocal, target_drum, target_music = batch\n",
    "        inputs = inputs.unsqueeze(1)  # Add channel dimension\n",
    "        \n",
    "        output = model(inputs)  # Forward pass\n",
    "\n",
    "        # Ensure output is exactly 441000 samples\n",
    "        target_length = 441000\n",
    "        if output.shape[-1] < target_length:\n",
    "            pad_size = target_length - output.shape[-1]\n",
    "            output = F.pad(output, (0, pad_size))\n",
    "\n",
    "        # Save outputs as audio files\n",
    "        current_batch_size = inputs.shape[0]  # Get actual batch size\n",
    "        for i in range(current_batch_size):\n",
    "            output_bass = remove_low_amplitude_noise(output[i, 0, :].cpu().numpy())\n",
    "            output_vocal = remove_low_amplitude_noise(output[i, 1, :].cpu().numpy())\n",
    "            output_drum = remove_low_amplitude_noise(output[i, 2, :].cpu().numpy())\n",
    "            output_music = remove_low_amplitude_noise(output[i, 3, :].cpu().numpy())\n",
    "            \n",
    "            sf.write(os.path.join(output_dir, f\"test_output_bass_{batch_idx*current_batch_size + i}.wav\"), output_bass, samplerate=44100)\n",
    "            sf.write(os.path.join(output_dir, f\"test_output_vocal_{batch_idx*current_batch_size + i}.wav\"), output_vocal, samplerate=44100)\n",
    "            sf.write(os.path.join(output_dir, f\"test_output_drum_{batch_idx*current_batch_size + i}.wav\"), output_drum, samplerate=44100)\n",
    "            sf.write(os.path.join(output_dir, f\"test_output_music_{batch_idx*current_batch_size + i}.wav\"), output_music, samplerate=44100)\n",
    "\n",
    "        # Compute batch loss and accumulate\n",
    "        batch_loss = (criterion(output[:, 0, :], target_bass) +\n",
    "                      criterion(output[:, 1, :], target_vocal) +\n",
    "                      criterion(output[:, 2, :], target_drum) +\n",
    "                      criterion(output[:, 3, :], target_music)).item()\n",
    "\n",
    "        val_loss += batch_loss / len(test_data_loader)  # Average over dataset\n",
    "\n",
    "print(f\"Validation Loss: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noise reduction and equalization (bandpass filter) applied, and audio saved.\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "from scipy.signal import butter, lfilter\n",
    "\n",
    "# Function for Butterworth Bandpass Filter\n",
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    nyquist = 0.5 * fs\n",
    "    low = lowcut / nyquist\n",
    "    high = highcut / nyquist\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return b, a\n",
    "\n",
    "# Function to apply bandpass filter\n",
    "def bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order)\n",
    "    y = lfilter(b, a, data)\n",
    "    return y\n",
    "\n",
    "# Function for simple noise reduction\n",
    "def noise_reduction(audio, sr):\n",
    "    # Simple noise reduction: split based on silence and remove the noise segments\n",
    "    noise_estimation = librosa.effects.split(audio, top_db=30)  # Adjust top_db as needed\n",
    "    clean_audio = np.concatenate([audio[start:end] for start, end in noise_estimation], axis=0)\n",
    "    return clean_audio\n",
    "\n",
    "# Load audio using librosa\n",
    "audio_path = r\"C:\\Users\\Mora siri\\Desktop\\output_audio_files_seconds\\test_output_vocal_1.wav\"\n",
    "audio, sr = librosa.load(audio_path, sr=44100)  # Use 44100 Hz sample rate\n",
    "\n",
    "# Step 1: Apply Noise Reduction\n",
    "clean_audio = noise_reduction(audio, sr)\n",
    "\n",
    "# Step 2: Apply Bandpass Filter (Isolate vocal frequencies between 500Hz and 3000Hz)\n",
    "filtered_audio = bandpass_filter(clean_audio, 500, 3000, sr)\n",
    "\n",
    "# Step 3: Save the processed audio (with both noise reduction and bandpass filter applied)\n",
    "output_path = r\"C:\\Users\\Mora siri\\Desktop\\output_audio_files_seconds\\processed_vocal_1.wav\"\n",
    "sf.write(output_path, filtered_audio, sr)\n",
    "\n",
    "print(\"Noise reduction and equalization (bandpass filter) applied, and audio saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
