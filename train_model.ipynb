{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import librosa\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset directory\n",
    "dataset_dir = \"/path/to/dataset/\"\n",
    "batch_size = 3  # You can change this value as needed\n",
    "segment_length =10*44100 # Fixed length for all audio samples (~1.5 sec at 44.1kHz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get file paths for each category\n",
    "def get_file_paths(folder_name):\n",
    "    path = os.path.join(dataset_dir, folder_name)\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Warning: {folder_name} folder not found!\")\n",
    "        return []\n",
    "    return sorted([os.path.join(path, f) for f in os.listdir(path)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_paths = get_file_paths(\"song\")\n",
    "bass_paths = get_file_paths(\"bass\")\n",
    "vocal_paths = get_file_paths(\"vocal\")\n",
    "drum_paths = get_file_paths(\"drum\")\n",
    "music_paths = get_file_paths(\"music\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "def load_audio(file_path, target_sr=44100, segment_length=441000):  \n",
    "    audio, sr = librosa.load(file_path, sr=target_sr, mono=True)\n",
    "    \n",
    "    # Pad if audio is shorter than segment length\n",
    "    if len(audio) < segment_length:\n",
    "        audio = np.pad(audio, (0, segment_length - len(audio)), mode='constant')\n",
    "\n",
    "    # Number of segments (each 10 seconds long)\n",
    "    num_segments = len(audio) // segment_length  \n",
    "    segments = []\n",
    "\n",
    "    for i in range(num_segments):\n",
    "        start = i * segment_length\n",
    "        end = start + segment_length\n",
    "        segment = audio[start:end]\n",
    "\n",
    "        # Get max amplitude of the segment\n",
    "        original_max = np.max(np.abs(segment)) if np.max(np.abs(segment)) > 0 else 1.0  \n",
    "\n",
    "        # Normalize the segment\n",
    "        segment = segment / original_max if original_max > 0 else segment\n",
    "        \n",
    "        segments.append(segment)\n",
    "\n",
    "    return np.array(segments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 60 samples for training.\n",
      "Loading dataset...\n",
      "Dataset loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Ensure dataset length consistency\n",
    "min_len = min(len(song_paths), len(bass_paths), len(vocal_paths), len(drum_paths), len(music_paths))\n",
    "print(f\"Using {min_len} samples for training.\")\n",
    "song_paths, bass_paths, vocal_paths, drum_paths, music_paths = (\n",
    "    song_paths[:min_len], bass_paths[:min_len], vocal_paths[:min_len], drum_paths[:min_len], music_paths[:min_len]\n",
    ")\n",
    "\n",
    "# Prepare dataset\n",
    "train_data = []  # Stores only the audio segments for training\n",
    "max_amplitudes = []  # Stores original max amplitudes for reconstruction\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "for i in range(min_len):\n",
    "    song_segments = load_audio(song_paths[i])\n",
    "    bass_segments = load_audio(bass_paths[i])\n",
    "    vocal_segments = load_audio(vocal_paths[i])\n",
    "    drum_segments= load_audio(drum_paths[i])\n",
    "    music_segments = load_audio(music_paths[i])\n",
    "    \n",
    "    min_segments = min(len(song_segments), len(bass_segments), len(vocal_segments), len(drum_segments), len(music_segments))\n",
    "\n",
    "    for j in range(min_segments):\n",
    "      train_data.append((\n",
    "        song_segments[j], \n",
    "        bass_segments[j], \n",
    "        vocal_segments[j], \n",
    "        drum_segments[j], \n",
    "        music_segments[j]\n",
    "    ))  \n",
    "\n",
    "\n",
    "       \n",
    "print(\"Dataset loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples in dataset: 216\n",
      "Each sample should have 5 elements (song, bass, vocal, drum, music): 5\n",
      "Example shapes:\n",
      "Song segment shape: (441000,)\n",
      "Bass segment shape: (441000,)\n",
      "Vocal segment shape: (441000,)\n",
      "Drum segment shape: (441000,)\n",
      "Music segment shape: (441000,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total samples in dataset: {len(train_data)}\")   \n",
    "print(f\"Each sample should have 5 elements (song, bass, vocal, drum, music): {len(train_data[0])}\")  \n",
    "\n",
    "# Print the shape of a few segments\n",
    "print(\"Example shapes:\")\n",
    "print(f\"Song segment shape: {train_data[0][0].shape}\")\n",
    "print(f\"Bass segment shape: {train_data[0][1].shape}\")\n",
    "print(f\"Vocal segment shape: {train_data[0][2].shape}\")\n",
    "print(f\"Drum segment shape: {train_data[0][3].shape}\")\n",
    "print(f\"Music segment shape: {train_data[0][4].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All segments have matching shapes\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):  # Checking first 5 samples\n",
    "    assert train_data[i][0].shape == train_data[i][1].shape == train_data[i][2].shape == train_data[i][3].shape == train_data[i][4].shape, f\"Mismatch at index {i}\"\n",
    "print(\"All segments have matching shapes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Samples: 151\n",
      "Validation Samples: 32\n",
      "Testing Samples: 33\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "# Define split ratios\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "# First split: Train (70%) and Temp (30%)\n",
    "train_set, temp_set = train_test_split(train_data, test_size=(val_ratio + test_ratio), random_state=42)\n",
    "\n",
    "# Second split: Validation (15%) and Test (15%)\n",
    "val_set, test_set = train_test_split(temp_set, test_size=(test_ratio / (val_ratio + test_ratio)), random_state=42)\n",
    "\n",
    "# Convert lists to PyTorch tensors\n",
    "train_dataset = TensorDataset(*[torch.tensor(np.array(d), dtype=torch.float32) for d in zip(*train_set)])\n",
    "val_dataset = TensorDataset(*[torch.tensor(np.array(d), dtype=torch.float32) for d in zip(*val_set)])\n",
    "test_dataset = TensorDataset(*[torch.tensor(np.array(d), dtype=torch.float32) for d in zip(*test_set)])\n",
    "\n",
    "# Prepare DataLoaders\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_data_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f\"Training Samples: {len(train_dataset)}\")\n",
    "print(f\"Validation Samples: {len(val_dataset)}\")\n",
    "print(f\"Testing Samples: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 151\n",
      "Number of testing samples: 33\n",
      "Number of training batches: 51\n",
      "Number of testing batches: 11\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of testing samples: {len(test_dataset)}\")\n",
    "print(f\"Number of training batches: {len(train_data_loader)}\")\n",
    "print(f\"Number of testing batches: {len(test_data_loader)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape: [(3, 441000), (3, 441000), (3, 441000), (3, 441000), (3, 441000)]\n"
     ]
    }
   ],
   "source": [
    "for batch in train_data_loader:\n",
    "    batch_shapes = [tuple(b.shape) for b in batch]\n",
    "    print(f\"Batch shape: {batch_shapes}\")\n",
    "    break  # Print only the first batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Constants\n",
    "num_sources = 4  # bass, vocal, drums, music\n",
    "\n",
    "class DemucsModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DemucsModel, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(1, 32, kernel_size=16, stride=4, padding=8),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, 64, kernel_size=16, stride=4, padding=8),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(64, 128, kernel_size=16, stride=4, padding=8),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(128, 256, kernel_size=16, stride=8, padding=8),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Bidirectional LSTM for temporal modeling\n",
    "        self.rnn = nn.LSTM(256, 256, batch_first=True, bidirectional=True)\n",
    "        self.lstm_fc = nn.Linear(512, 256)  # Merge bidirectional outputs\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose1d(256, 128, kernel_size=16, stride=8, padding=8, output_padding=2),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(128, 64, kernel_size=16, stride=4, padding=8, output_padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(64, 32, kernel_size=16, stride=4, padding=8, output_padding=1),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(32, num_sources, kernel_size=16, stride=4, padding=8, output_padding=1),\n",
    "            nn.Tanh(),  # Keeps output in range (-1, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)  \n",
    "        x = x.permute(0, 2, 1)  # Change to (batch, time, channels) for LSTM\n",
    "        x, _ = self.rnn(x)  \n",
    "        x = self.lstm_fc(x)  \n",
    "        x = self.decoder(x.permute(0, 2, 1))  # Change back to (batch, channels, time)\n",
    "        return x\n",
    "\n",
    "# Initialize Model\n",
    "model = DemucsModel()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DemucsModel(\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv1d(1, 32, kernel_size=(16,), stride=(4,), padding=(8,))\n",
      "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Conv1d(32, 64, kernel_size=(16,), stride=(4,), padding=(8,))\n",
      "    (4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "    (6): Conv1d(64, 128, kernel_size=(16,), stride=(4,), padding=(8,))\n",
      "    (7): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU()\n",
      "    (9): Conv1d(128, 256, kernel_size=(16,), stride=(8,), padding=(8,))\n",
      "    (10): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): ReLU()\n",
      "  )\n",
      "  (rnn): LSTM(256, 256, batch_first=True, bidirectional=True)\n",
      "  (lstm_fc): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (decoder): Sequential(\n",
      "    (0): ConvTranspose1d(256, 128, kernel_size=(16,), stride=(8,), padding=(8,), output_padding=(2,))\n",
      "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): ConvTranspose1d(128, 64, kernel_size=(16,), stride=(4,), padding=(8,), output_padding=(1,))\n",
      "    (4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "    (6): ConvTranspose1d(64, 32, kernel_size=(16,), stride=(4,), padding=(8,), output_padding=(1,))\n",
      "    (7): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU()\n",
      "    (9): ConvTranspose1d(32, 4, kernel_size=(16,), stride=(4,), padding=(8,), output_padding=(1,))\n",
      "    (10): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def si_snr_loss(pred, target, eps=1e-8):\n",
    "    target_energy = torch.sum(target**2, dim=-1, keepdim=True) + eps\n",
    "    scale = torch.sum(target * pred, dim=-1, keepdim=True) / target_energy\n",
    "    target_proj = scale * target\n",
    "    noise = pred - target_proj\n",
    "\n",
    "    si_snr = torch.sum(target_proj**2, dim=-1) / (torch.sum(noise**2, dim=-1) + eps)\n",
    "    si_snr = 10 * torch.log10(si_snr + eps)\n",
    "    \n",
    "    return -si_snr.mean()\n",
    "\n",
    "criterion = si_snr_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 1/100 - Train Loss: 42.8301, Val Loss: 53.4457, Avg SDR: -8.1676\n",
      "Epoch 2/100 - Train Loss: 31.0846, Val Loss: 36.6078, Avg SDR: -8.0827\n",
      "Epoch 3/100 - Train Loss: 23.6489, Val Loss: 28.7485, Avg SDR: -8.0204\n",
      "Epoch 4/100 - Train Loss: 22.4305, Val Loss: 26.9486, Avg SDR: -8.1171\n",
      "Epoch 5/100 - Train Loss: 21.1618, Val Loss: 29.3575, Avg SDR: -7.9763\n",
      "Epoch 6/100 - Train Loss: 20.6698, Val Loss: 26.1854, Avg SDR: -8.1126\n",
      "Epoch 7/100 - Train Loss: 17.0494, Val Loss: 25.2461, Avg SDR: -8.0441\n",
      "Epoch 8/100 - Train Loss: 15.8840, Val Loss: 23.9656, Avg SDR: -7.9517\n",
      "Epoch 9/100 - Train Loss: 15.0103, Val Loss: 23.0506, Avg SDR: -7.8187\n",
      "Epoch 10/100 - Train Loss: 14.2911, Val Loss: 22.8091, Avg SDR: -7.6956\n",
      "Epoch 11/100 - Train Loss: 13.6515, Val Loss: 22.5939, Avg SDR: -7.6596\n",
      "Epoch 12/100 - Train Loss: 13.1466, Val Loss: 21.4359, Avg SDR: -7.5673\n",
      "Epoch 13/100 - Train Loss: 12.6154, Val Loss: 21.1010, Avg SDR: -7.1917\n",
      "Epoch 14/100 - Train Loss: 12.1697, Val Loss: 20.6823, Avg SDR: -7.1542\n",
      "Epoch 15/100 - Train Loss: 11.6461, Val Loss: 20.8361, Avg SDR: -6.8660\n",
      "Epoch 16/100 - Train Loss: 11.3626, Val Loss: 21.0618, Avg SDR: -6.8283\n",
      "Epoch 17/100 - Train Loss: 10.9026, Val Loss: 19.9438, Avg SDR: -6.7003\n",
      "Epoch 18/100 - Train Loss: 10.5144, Val Loss: 19.9984, Avg SDR: -6.6089\n",
      "Epoch 19/100 - Train Loss: 10.0173, Val Loss: 19.3914, Avg SDR: -6.4436\n",
      "Epoch 20/100 - Train Loss: 9.7442, Val Loss: 20.7845, Avg SDR: -6.2604\n",
      "Epoch 21/100 - Train Loss: 9.4364, Val Loss: 19.5034, Avg SDR: -6.1919\n",
      "Epoch 22/100 - Train Loss: 9.0689, Val Loss: 19.4149, Avg SDR: -6.1827\n",
      "Epoch 23/100 - Train Loss: 8.7759, Val Loss: 19.2514, Avg SDR: -5.9629\n",
      "Epoch 24/100 - Train Loss: 8.4575, Val Loss: 19.4648, Avg SDR: -5.9944\n",
      "Epoch 25/100 - Train Loss: 8.0343, Val Loss: 19.8549, Avg SDR: -5.7886\n",
      "Epoch 26/100 - Train Loss: 7.6780, Val Loss: 18.9397, Avg SDR: -5.7702\n",
      "Epoch 27/100 - Train Loss: 7.4712, Val Loss: 18.8163, Avg SDR: -5.5965\n",
      "Epoch 28/100 - Train Loss: 7.0301, Val Loss: 18.6033, Avg SDR: -5.4766\n",
      "Epoch 29/100 - Train Loss: 6.6612, Val Loss: 19.1334, Avg SDR: -5.1934\n",
      "Epoch 30/100 - Train Loss: 6.4754, Val Loss: 19.2728, Avg SDR: -5.1680\n",
      "Epoch 31/100 - Train Loss: 6.2115, Val Loss: 19.5081, Avg SDR: -5.2099\n",
      "Epoch 32/100 - Train Loss: 6.0401, Val Loss: 19.4322, Avg SDR: -5.0541\n",
      "Epoch 33/100 - Train Loss: 5.9723, Val Loss: 19.2050, Avg SDR: -4.9634\n",
      "Epoch 34/100 - Train Loss: 5.5611, Val Loss: 20.4381, Avg SDR: -4.9485\n",
      "Epoch 35/100 - Train Loss: 5.3862, Val Loss: 19.5698, Avg SDR: -4.6676\n",
      "Epoch 36/100 - Train Loss: 5.2288, Val Loss: 18.7018, Avg SDR: -4.8224\n",
      "Epoch 37/100 - Train Loss: 5.0403, Val Loss: 18.8622, Avg SDR: -4.4460\n",
      "Epoch 38/100 - Train Loss: 4.7133, Val Loss: 20.7400, Avg SDR: -4.1835\n",
      "Epoch 39/100 - Train Loss: 4.5424, Val Loss: 19.8347, Avg SDR: -4.4179\n",
      "Epoch 40/100 - Train Loss: 4.1373, Val Loss: 18.6589, Avg SDR: -4.1630\n",
      "Epoch 41/100 - Train Loss: 4.1387, Val Loss: 18.7217, Avg SDR: -3.9387\n",
      "Epoch 42/100 - Train Loss: 4.1021, Val Loss: 18.1089, Avg SDR: -3.9269\n",
      "Epoch 43/100 - Train Loss: 3.7674, Val Loss: 20.4273, Avg SDR: -3.5496\n",
      "Epoch 44/100 - Train Loss: 3.6703, Val Loss: 18.8276, Avg SDR: -3.4137\n",
      "Epoch 45/100 - Train Loss: 3.3956, Val Loss: 18.5718, Avg SDR: -3.3116\n",
      "Epoch 46/100 - Train Loss: 3.1093, Val Loss: 18.6812, Avg SDR: -3.1216\n",
      "Epoch 47/100 - Train Loss: 2.9108, Val Loss: 19.4467, Avg SDR: -2.9519\n",
      "Epoch 48/100 - Train Loss: 2.6469, Val Loss: 19.5179, Avg SDR: -2.7365\n",
      "Epoch 49/100 - Train Loss: 2.4134, Val Loss: 19.5277, Avg SDR: -2.7214\n",
      "Epoch 50/100 - Train Loss: 2.1156, Val Loss: 18.4852, Avg SDR: -2.7442\n",
      "Epoch 51/100 - Train Loss: 1.9082, Val Loss: 19.0327, Avg SDR: -2.3463\n",
      "Epoch 52/100 - Train Loss: 1.7290, Val Loss: 17.6416, Avg SDR: -2.4781\n",
      "Epoch 53/100 - Train Loss: 2.1676, Val Loss: 19.1445, Avg SDR: -2.4915\n",
      "Epoch 54/100 - Train Loss: 1.8454, Val Loss: 18.5205, Avg SDR: -2.5046\n",
      "Epoch 55/100 - Train Loss: 1.3357, Val Loss: 18.5684, Avg SDR: -2.3998\n",
      "Epoch 56/100 - Train Loss: 1.2051, Val Loss: 18.0131, Avg SDR: -2.2446\n",
      "Epoch 57/100 - Train Loss: 1.1153, Val Loss: 18.3389, Avg SDR: -2.2544\n",
      "Epoch 58/100 - Train Loss: 0.8945, Val Loss: 18.6363, Avg SDR: -1.8895\n",
      "Epoch 59/100 - Train Loss: 1.1125, Val Loss: 19.1509, Avg SDR: -1.8885\n",
      "Epoch 60/100 - Train Loss: 1.2425, Val Loss: 18.4422, Avg SDR: -2.0676\n",
      "Epoch 61/100 - Train Loss: 0.9711, Val Loss: 19.0718, Avg SDR: -2.2775\n",
      "Epoch 62/100 - Train Loss: 0.9520, Val Loss: 18.7728, Avg SDR: -2.1599\n",
      "Epoch 63/100 - Train Loss: 0.4792, Val Loss: 19.1768, Avg SDR: -2.2164\n",
      "Epoch 64/100 - Train Loss: 0.4417, Val Loss: 18.0666, Avg SDR: -1.8369\n",
      "Epoch 65/100 - Train Loss: 0.0745, Val Loss: 18.5868, Avg SDR: -1.8385\n",
      "Epoch 66/100 - Train Loss: -0.0344, Val Loss: 18.5048, Avg SDR: -2.0321\n",
      "Epoch 67/100 - Train Loss: -0.0037, Val Loss: 19.3343, Avg SDR: -1.8248\n",
      "Epoch 68/100 - Train Loss: -0.1358, Val Loss: 18.0107, Avg SDR: -1.6794\n",
      "Epoch 69/100 - Train Loss: -0.3425, Val Loss: 18.1130, Avg SDR: -1.9679\n",
      "Epoch 70/100 - Train Loss: -0.3465, Val Loss: 19.1873, Avg SDR: -1.4388\n",
      "Epoch 71/100 - Train Loss: -0.3753, Val Loss: 18.5442, Avg SDR: -1.5879\n",
      "Epoch 72/100 - Train Loss: -0.5078, Val Loss: 17.7099, Avg SDR: -1.7378\n",
      "Epoch 73/100 - Train Loss: -0.5565, Val Loss: 18.9154, Avg SDR: -1.5275\n",
      "Epoch 74/100 - Train Loss: -0.7930, Val Loss: 18.3216, Avg SDR: -1.4617\n",
      "Epoch 75/100 - Train Loss: -0.9310, Val Loss: 18.8257, Avg SDR: -1.9388\n",
      "Epoch 76/100 - Train Loss: -0.8173, Val Loss: 18.9350, Avg SDR: -1.4249\n",
      "Epoch 77/100 - Train Loss: -0.5300, Val Loss: 17.8903, Avg SDR: -1.7603\n",
      "Epoch 78/100 - Train Loss: -0.6056, Val Loss: 20.2343, Avg SDR: -1.8910\n",
      "Epoch 79/100 - Train Loss: -0.7518, Val Loss: 18.9377, Avg SDR: -1.9034\n",
      "Epoch 80/100 - Train Loss: -1.0145, Val Loss: 18.5720, Avg SDR: -1.4038\n",
      "Epoch 81/100 - Train Loss: -0.9438, Val Loss: 20.1038, Avg SDR: -1.6649\n",
      "Epoch 82/100 - Train Loss: -1.3184, Val Loss: 18.2091, Avg SDR: -0.9941\n",
      "Epoch 83/100 - Train Loss: -1.5657, Val Loss: 18.7527, Avg SDR: -1.4091\n",
      "Epoch 84/100 - Train Loss: -1.7892, Val Loss: 19.6022, Avg SDR: -1.4593\n",
      "Epoch 85/100 - Train Loss: -1.9436, Val Loss: 19.3763, Avg SDR: -1.6303\n",
      "Epoch 86/100 - Train Loss: -1.8382, Val Loss: 18.5564, Avg SDR: -1.3202\n",
      "Epoch 87/100 - Train Loss: -1.9909, Val Loss: 18.5811, Avg SDR: -1.5598\n",
      "Epoch 88/100 - Train Loss: -1.9971, Val Loss: 19.4406, Avg SDR: -1.4087\n",
      "Epoch 89/100 - Train Loss: -2.0568, Val Loss: 19.7179, Avg SDR: -1.4702\n",
      "Epoch 90/100 - Train Loss: -1.8632, Val Loss: 18.0049, Avg SDR: -1.5614\n",
      "Epoch 91/100 - Train Loss: -2.2764, Val Loss: 17.9578, Avg SDR: -1.6012\n",
      "Epoch 92/100 - Train Loss: -2.4517, Val Loss: 18.2390, Avg SDR: -1.2099\n",
      "Epoch 93/100 - Train Loss: -2.4853, Val Loss: 19.4760, Avg SDR: -1.3922\n",
      "Epoch 94/100 - Train Loss: -2.4537, Val Loss: 18.8108, Avg SDR: -1.0719\n",
      "Epoch 95/100 - Train Loss: -1.9537, Val Loss: 19.8965, Avg SDR: -1.2158\n",
      "Epoch 96/100 - Train Loss: -2.2057, Val Loss: 19.6625, Avg SDR: -1.5516\n",
      "Epoch 97/100 - Train Loss: -2.2240, Val Loss: 18.3103, Avg SDR: -1.2978\n",
      "Epoch 98/100 - Train Loss: -1.6241, Val Loss: 19.8940, Avg SDR: -1.6028\n",
      "Epoch 99/100 - Train Loss: -1.5996, Val Loss: 19.6550, Avg SDR: -1.3646\n",
      "Epoch 100/100 - Train Loss: -2.0181, Val Loss: 18.9593, Avg SDR: -1.6826\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import csv\n",
    "\n",
    "# Ensure target length consistency\n",
    "target_length = 441000\n",
    "\n",
    "# File to store loss and SDR values\n",
    "csv_filename = \"training_metrics.csv\"\n",
    "with open(csv_filename, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Epoch\", \"Train_Loss\", \"Val_Loss\", \"Avg_SDR\"])\n",
    "\n",
    "def calculate_sdr(predicted, target):\n",
    "    # Avoid log issues with small values\n",
    "    eps = 1e-8\n",
    "    return 10 * torch.log10(torch.sum(target**2) / (torch.sum((target - predicted)**2) + eps))\n",
    "\n",
    "print(\"Starting training...\")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_train_loss = 0\n",
    "    epoch_sdr_values = []\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_data_loader):\n",
    "        inputs, target_bass, target_vocal, target_drum, target_music = batch\n",
    "        inputs = inputs.unsqueeze(1)\n",
    "\n",
    "        # Padding inputs and targets\n",
    "        inputs = F.pad(inputs, (0, target_length - inputs.shape[-1]))  \n",
    "        target_bass = F.pad(target_bass, (0, target_length - target_bass.shape[-1]))\n",
    "        target_vocal = F.pad(target_vocal, (0, target_length - target_vocal.shape[-1]))\n",
    "        target_drum = F.pad(target_drum, (0, target_length - target_drum.shape[-1]))\n",
    "        target_music = F.pad(target_music, (0, target_length - target_music.shape[-1]))\n",
    "\n",
    "        targets = torch.stack([target_bass, target_vocal, target_drum, target_music], dim=1)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        if outputs.shape[-1] < target_length:\n",
    "            outputs = F.pad(outputs, (0, target_length - outputs.shape[-1]))\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_train_loss += loss.item()\n",
    "        \n",
    "        # Compute SDR for the batch\n",
    "        batch_sdr = calculate_sdr(outputs, targets).item()\n",
    "        epoch_sdr_values.append(batch_sdr)\n",
    "    \n",
    "    # Compute average train loss and SDR\n",
    "    avg_train_loss = epoch_train_loss / len(train_data_loader)\n",
    "    avg_sdr = sum(epoch_sdr_values) / len(epoch_sdr_values)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    epoch_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_data_loader:\n",
    "            inputs, target_bass, target_vocal, target_drum, target_music = batch\n",
    "            inputs = inputs.unsqueeze(1)\n",
    "            \n",
    "            inputs = F.pad(inputs, (0, target_length - inputs.shape[-1]))  \n",
    "            target_bass = F.pad(target_bass, (0, target_length - target_bass.shape[-1]))\n",
    "            target_vocal = F.pad(target_vocal, (0, target_length - target_vocal.shape[-1]))\n",
    "            target_drum = F.pad(target_drum, (0, target_length - target_drum.shape[-1]))\n",
    "            target_music = F.pad(target_music, (0, target_length - target_music.shape[-1]))\n",
    "\n",
    "            targets = torch.stack([target_bass, target_vocal, target_drum, target_music], dim=1)\n",
    "            outputs = model(inputs)\n",
    "            if outputs.shape[-1] < target_length:\n",
    "                outputs = F.pad(outputs, (0, target_length - outputs.shape[-1]))\n",
    "            \n",
    "            val_loss = criterion(outputs, targets)\n",
    "            epoch_val_loss += val_loss.item()\n",
    "    \n",
    "    avg_val_loss = epoch_val_loss / len(val_data_loader)\n",
    "    \n",
    "    # Save to CSV\n",
    "    with open(csv_filename, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([epoch + 1, avg_train_loss, avg_val_loss, avg_sdr])\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Avg SDR: {avg_sdr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted shape: torch.Size([3, 4, 441000])\n",
      "Target shape: torch.Size([3, 441000])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Predicted shape: {outputs.shape}\")\n",
    "print(f\"Target shape: {target_bass.shape}\")  # Target shape should be same as predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"audio_separation_model_2.pth\")\n",
    "print(\"Model saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def remove_low_amplitude_noise(audio, threshold_ratio=0.07):\n",
    "    max_amplitude = np.max(np.abs(audio))\n",
    "    threshold = max_amplitude * threshold_ratio\n",
    "    audio_denoised = np.where(np.abs(audio) > threshold, audio, 0)\n",
    "    return audio_denoised\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting validation...\n",
      "Validation Loss: 61.3517\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import soundfile as sf\n",
    "\n",
    "print(\"Starting validation...\")\n",
    "model.eval()  # Set model to evaluation mode\n",
    "val_loss = 0\n",
    "output_dir = \"/path/to/output_directory/\"  # Output directory\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "with torch.no_grad():  # No need to compute gradients during validation\n",
    "    for batch_idx, batch in enumerate(test_data_loader):\n",
    "        inputs, target_bass, target_vocal, target_drum, target_music = batch\n",
    "        inputs = inputs.unsqueeze(1)  # Add channel dimension\n",
    "        \n",
    "        output = model(inputs)  # Forward pass\n",
    "\n",
    "        # Ensure output is exactly 441000 samples\n",
    "        target_length = 441000\n",
    "        if output.shape[-1] < target_length:\n",
    "            pad_size = target_length - output.shape[-1]\n",
    "            output = F.pad(output, (0, pad_size))\n",
    "\n",
    "        # Save outputs as audio files\n",
    "        current_batch_size = inputs.shape[0]  # Get actual batch size\n",
    "        for i in range(current_batch_size):\n",
    "            output_bass = remove_low_amplitude_noise(output[i, 0, :].cpu().numpy())\n",
    "            output_vocal = remove_low_amplitude_noise(output[i, 1, :].cpu().numpy())\n",
    "            output_drum = remove_low_amplitude_noise(output[i, 2, :].cpu().numpy())\n",
    "            output_music = remove_low_amplitude_noise(output[i, 3, :].cpu().numpy())\n",
    "            \n",
    "            sf.write(os.path.join(output_dir, f\"test_output_bass_{batch_idx*current_batch_size + i}.wav\"), output_bass, samplerate=44100)\n",
    "            sf.write(os.path.join(output_dir, f\"test_output_vocal_{batch_idx*current_batch_size + i}.wav\"), output_vocal, samplerate=44100)\n",
    "            sf.write(os.path.join(output_dir, f\"test_output_drum_{batch_idx*current_batch_size + i}.wav\"), output_drum, samplerate=44100)\n",
    "            sf.write(os.path.join(output_dir, f\"test_output_music_{batch_idx*current_batch_size + i}.wav\"), output_music, samplerate=44100)\n",
    "\n",
    "        # Compute batch loss and accumulate\n",
    "        batch_loss = (criterion(output[:, 0, :], target_bass) +\n",
    "                      criterion(output[:, 1, :], target_vocal) +\n",
    "                      criterion(output[:, 2, :], target_drum) +\n",
    "                      criterion(output[:, 3, :], target_music)).item()\n",
    "\n",
    "        val_loss += batch_loss / len(test_data_loader)  # Average over dataset\n",
    "\n",
    "print(f\"Validation Loss: {val_loss:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
